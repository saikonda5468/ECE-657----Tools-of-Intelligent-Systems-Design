{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e5d4bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiko\\AppData\\Local\\Temp\\ipykernel_35112\\2451931313.py:104: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  num_correct = sum(np.all(true_label == pred_label) for true_label, pred_label in zip(y_true, y_pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10 Loss function value: 0.7731534068008766 Training Accuracy: 0.8425491087209008 Validation Accuracy: 0.8381818181818181\n",
      "epoch = 20 Loss function value: 0.5267145567140168 Training Accuracy: 0.8736050093420189 Validation Accuracy: 0.8660606060606061\n",
      "epoch = 30 Loss function value: 0.43510915631697317 Training Accuracy: 0.8906731303337878 Validation Accuracy: 0.8840404040404041\n",
      "epoch = 40 Loss function value: 0.38282893444267585 Training Accuracy: 0.9004191284148866 Validation Accuracy: 0.8923232323232323\n",
      "epoch = 50 Loss function value: 0.34627712769381036 Training Accuracy: 0.911225571883048 Validation Accuracy: 0.8985858585858586\n",
      "epoch = 60 Loss function value: 0.3184621987069905 Training Accuracy: 0.9189011765894056 Validation Accuracy: 0.9066666666666666\n",
      "epoch = 70 Loss function value: 0.2988521216466954 Training Accuracy: 0.9229914659395041 Validation Accuracy: 0.9109090909090909\n",
      "epoch = 80 Loss function value: 0.2843451272495522 Training Accuracy: 0.9261223047013079 Validation Accuracy: 0.915959595959596\n",
      "epoch = 90 Loss function value: 0.2720774114838826 Training Accuracy: 0.9288491642680402 Validation Accuracy: 0.9175757575757576\n",
      "epoch = 100 Loss function value: 0.2616527332559589 Training Accuracy: 0.9303640862495581 Validation Accuracy: 0.9204040404040404\n",
      "epoch = 110 Loss function value: 0.2523413138948342 Training Accuracy: 0.931980003029844 Validation Accuracy: 0.9234343434343434\n",
      "epoch = 120 Loss function value: 0.24307109190065562 Training Accuracy: 0.9343028834015048 Validation Accuracy: 0.9252525252525252\n",
      "epoch = 130 Loss function value: 0.23464230324907612 Training Accuracy: 0.9361207897793263 Validation Accuracy: 0.9276767676767677\n",
      "epoch = 140 Loss function value: 0.2272896915890379 Training Accuracy: 0.9373327273645408 Validation Accuracy: 0.9290909090909091\n",
      "epoch = 150 Loss function value: 0.22002737831469646 Training Accuracy: 0.9392011311417462 Validation Accuracy: 0.9317171717171717\n",
      "epoch = 160 Loss function value: 0.21311108820756633 Training Accuracy: 0.9412210271171034 Validation Accuracy: 0.9345454545454546\n",
      "epoch = 170 Loss function value: 0.20706272654639196 Training Accuracy: 0.94233196990355 Validation Accuracy: 0.934949494949495\n",
      "epoch = 180 Loss function value: 0.20153796354238518 Training Accuracy: 0.9431904256930768 Validation Accuracy: 0.9365656565656566\n",
      "epoch = 190 Loss function value: 0.19673409464103342 Training Accuracy: 0.9439983840832197 Validation Accuracy: 0.937979797979798\n",
      "epoch = 200 Loss function value: 0.19255756366551224 Training Accuracy: 0.9449073372721305 Validation Accuracy: 0.9383838383838384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "\n",
    "#class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_nodes, num_classes, learning_rate, num_epochs):\n",
    "        #initializing the paramaters\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.W1 = None\n",
    "        self.b1 = None\n",
    "        self.W2 = None\n",
    "        self.b2 = None\n",
    "    #relu function which is used as activation function\n",
    "    @staticmethod\n",
    "    def relu_func(x):\n",
    "        return np.maximum(0, x)\n",
    "    #derivative of relu function\n",
    "    @staticmethod\n",
    "    def relu_drv(x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    #softmax function used to convert the output of NN to categorical data\n",
    "    @staticmethod\n",
    "    def softmax_func(x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot_enc(arr):\n",
    "        argmax_labels = np.argmax(arr, axis=1)\n",
    "        encoded_arr = pd.get_dummies(argmax_labels)\n",
    "        return encoded_arr.values\n",
    "\n",
    "\n",
    "    #loss function to determine the error\n",
    "    @staticmethod\n",
    "    def cross_ent(original_label, predict_label):\n",
    "        m = original_label.shape[0]  # Number of training examples\n",
    "        cost = - np.sum(np.multiply(original_label, np.log(predict_label))) / m\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "    #updating the weight after performing back propagation and getting gradients\n",
    "    def update_weights(self, weights, costs, train_data):\n",
    "        updated_weights = []\n",
    "        self.features = train_data.shape[1]\n",
    "        self.samples = len(train_data)\n",
    "\n",
    "        for weight, cost in zip(weights, costs):\n",
    "            #checking whether the dimension match and perfoming the updation.\n",
    "            # w = w - n*dw\n",
    "            if cost.shape == (self.features, self.hidden_nodes):\n",
    "                updated_weight = weight - self.learning_rate * cost\n",
    "            elif cost.shape == (self.hidden_nodes,):\n",
    "                updated_weight = weight - self.learning_rate * cost\n",
    "            elif cost.shape == (self.samples, self.hidden_nodes):\n",
    "                updated_weight = weight - self.learning_rate * cost.sum(axis=0)\n",
    "            elif cost.shape == (self.samples, self.num_classes):\n",
    "                updated_weight = weight - self.learning_rate * cost.sum(axis=0)\n",
    "            else:\n",
    "                updated_weight = weight\n",
    "            #updated weights\n",
    "            updated_weights.append(updated_weight)\n",
    "\n",
    "        return updated_weights\n",
    "\n",
    "\n",
    "    #intializing random values for weights and bias\n",
    "    def initialize_weights(self, train_data):\n",
    "        np.random.seed(0)\n",
    "        self.features = train_data.shape[1]\n",
    "        self.W1 = np.random.randn(self.features, self.hidden_nodes)\n",
    "        self.b1 = np.random.randn(self.hidden_nodes)\n",
    "        self.W2 = np.random.randn(self.hidden_nodes, self.num_classes)\n",
    "        self.b2 = np.random.randn(self.num_classes)\n",
    "    #performing forward propagation\n",
    "    def forward_prop(self, inp_data):\n",
    "        #z1=W1*x+b1\n",
    "        Z1 = np.dot(inp_data, self.W1) + self.b1\n",
    "        #A1 = relu(Z1)\n",
    "        A1 = self.relu_func(Z1)\n",
    "        #z2=W2*x+b2\n",
    "        Z2 = np.dot(A1, self.W2) + self.b2\n",
    "        #A2 = relu(Z2)\n",
    "        A2 = self.softmax_func(Z2)\n",
    "        return A2, A1, Z1\n",
    "    #Backpropagation\n",
    "    def backward_prop(self, X_train, y_train, net_hidden, act_hidden, weight_output, act_output):\n",
    "        m = X_train.shape[0]  # Number of training examples\n",
    "        #finding gradient descent values \n",
    "        dZ2 = act_output - y_train\n",
    "        dW2 = (1 / m) * np.dot(act_hidden.T, dZ2)\n",
    "        db2 = dZ2\n",
    "        dZ1 = np.multiply(np.dot(dZ2, weight_output.T), self.relu_drv(net_hidden))\n",
    "        dW1 = np.dot(X_train.T, self.relu_drv(net_hidden) * dZ1)\n",
    "        db1 = (dZ1 * self.relu_drv(net_hidden))\n",
    "        return dW2, db2, dW1, db1\n",
    "\n",
    "\n",
    "    #accuracy \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        if len(y_true) != len(y_pred):\n",
    "            print('Size does not match.')\n",
    "            return 0\n",
    "    \n",
    "        num_samples = len(y_true)\n",
    "        num_correct = sum(np.all(true_label == pred_label) for true_label, pred_label in zip(y_true, y_pred))#comparing the values and adding of the labels\n",
    "        accuracy = num_correct / num_samples\n",
    "        #the number of correct predictions divided by the total number of predictions made by the model\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def fit(self, train_data, train_labels, val_data, val_labels):\n",
    "        #initializing the weights and bias\n",
    "        self.initialize_weights(train_data)\n",
    "\n",
    "        #looping untill we the better accuracy\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            #performing forward propagation and getting ouptut values of activation function\n",
    "            act_output, act_hidden, net_hidden = self.forward_prop(train_data)\n",
    "            #performing the backward propagation and getting the gradient values\n",
    "            wto_grad, bo_grad, wth_grad, bh_grad = self.backward_prop(train_data, train_labels, net_hidden, act_hidden,\n",
    "                                                 self.W2, act_output)\n",
    "            #updating the weight using gradient descent\n",
    "            self.W1, self.b1, self.W2, self.b2 = self.update_weights([self.W1, self.b1, self.W2, self.b2],\n",
    "                                                         [wth_grad, bh_grad, wto_grad, bo_grad],\n",
    "                                                         train_data)\n",
    "\n",
    "            #calculating loss values\n",
    "            Loss = self.cross_ent(train_labels, act_output)\n",
    "            \n",
    "            #predicting the labels for train and val data\n",
    "            self.y_pred, _, _ = self.forward_prop(val_data)\n",
    "            # One hot encoding the prediction\n",
    "            self.y_pred_enc = self.one_hot_enc(self.y_pred)\n",
    "            # Calculating the accuracy\n",
    "            val_acc = self.accuracy(val_labels, self.y_pred_enc)\n",
    "            train_acc = self.accuracy(train_labels, self.one_hot_enc(act_output))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch =', epoch, 'Loss function value:', Loss, 'Training Accuracy:', train_acc, 'Validation Accuracy:', val_acc)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#spliting the data into train and Validation\n",
    "def training_data(df, encoded_labels, train_ratio, val_ratio):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    indices = np.arange(len(df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    shuffled_df = df.iloc[indices]\n",
    "    shuffled_labels = encoded_labels.iloc[indices]\n",
    "\n",
    "    num_examples = len(df)\n",
    "    num_train = int(train_ratio * num_examples)\n",
    "    num_val = int(val_ratio * num_examples)\n",
    "\n",
    "    train_data = shuffled_df[:num_train]\n",
    "    val_data = shuffled_df[num_train:num_train + num_val]\n",
    "    train_labels = shuffled_labels[:num_train]\n",
    "    val_labels = shuffled_labels[num_train:num_train + num_val]\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels\n",
    "\n",
    "# Usage example:\n",
    "np.random.seed(0)\n",
    "values = []\n",
    "for i in range(784):\n",
    "    values.append(i)\n",
    "df = pd.read_csv('C:/Users/saiko/OneDrive/Desktop/657/Assign-1/train_data.csv', names=values)\n",
    "label = pd.read_csv('C:/Users/saiko/OneDrive/Desktop/657/Assign-1/train_labels.csv',\n",
    "                    names=[i for i in range(0, 4)])\n",
    "encoded_labels = pd.get_dummies(label)\n",
    "train_data, train_labels, val_data, val_labels = training_data(df, encoded_labels, 0.8, 0.2)\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n",
    "\n",
    "hiddenlyr_nodes = 35\n",
    "num_classes = 4\n",
    "learning_rate = 0.0001\n",
    "num_epoch = 200\n",
    "#calling the model\n",
    "\n",
    "nn = NeuralNetwork(hiddenlyr_nodes, num_classes, learning_rate, num_epoch)\n",
    "nn.fit(train_data, train_labels, val_data, val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667cca1c",
   "metadata": {},
   "source": [
    "# Test data should be given here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b5d51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data prediction\n",
    "test_data = pd.read_csv(\"test data location goes here\")\n",
    "def predict_labels(test_data, neural_network):\n",
    "    A2, _, _ = neural_network.forward_prop(test_data)\n",
    "    labels = np.argmax(A2, axis=1)  # Get the index of the maximum activation for each sample\n",
    "    encoded_labels = np.eye(neural_network.num_classes)[labels]  # One-hot encode the labels\n",
    "    return encoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3605ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4945</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4947</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4948</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4949</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4950 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3\n",
       "0     0.0  1.0  0.0  0.0\n",
       "1     1.0  0.0  0.0  0.0\n",
       "2     0.0  0.0  1.0  0.0\n",
       "3     1.0  0.0  0.0  0.0\n",
       "4     0.0  0.0  1.0  0.0\n",
       "...   ...  ...  ...  ...\n",
       "4945  0.0  0.0  1.0  0.0\n",
       "4946  1.0  0.0  0.0  0.0\n",
       "4947  0.0  0.0  1.0  0.0\n",
       "4948  0.0  0.0  0.0  1.0\n",
       "4949  0.0  0.0  0.0  1.0\n",
       "\n",
       "[4950 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have loaded the test dataset into 'test_data'\n",
    "predicted_labels = predict_labels(test_data, nn)\n",
    "predicted_labels = pd.DataFrame(predicted_labels)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03867d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
